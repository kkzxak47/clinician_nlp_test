{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_text = \"\"\"Doctor: How are you Miss G? \n",
    "Patient: I am good doctor, thank you for asking. \n",
    "Doctor: So, tell me what is going on?\n",
    "Patient: I have this ear pain and headache for some time. It's better than before but I still want to get it checked. \n",
    "Doctor: Okay, when exactly did it start?\n",
    "Patient: Um, almost three weeks ago. I am having difficulty hearing. I also feel this pressure on the left side of my sinus causing tooth pain. I went to my dentist yesterday, but my teeth are fine. \n",
    "Doctor: Okay, do you have headache now?\n",
    "Patient: No, just ear pain and this jaw pain on the left side. \n",
    "Doctor: Any fever, cough, sore throat, or any cold like symptoms? \n",
    "Patient: No, but I have a sinus problem and I suffer from chronic left sided headache.\n",
    "Doctor: How old are you?\n",
    "Patient: Oh, I am forty nine.\n",
    "Doctor: Hm, so are you taking any medications for your pain?\n",
    "Patient: No, currently I am just using Cutivate for my eczema. It has helped me a lot, I do need a refill for it. \n",
    "Doctor: Okay I will send a prescription for it to your pharmacy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invain/.virtualenvs/spacy/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/invain/.virtualenvs/spacy/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/invain/.virtualenvs/spacy/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(dialog_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss G PERSON\n",
      "almost three weeks ago DATE\n",
      "yesterday DATE\n",
      "Cutivate PRODUCT\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"\"\"Patient: I just had few questions. Can you tell me about my diagnosis?\n",
    "Doctor: Sure. It's called Serotonin syndrome, ma'am. After careful evaluation of your labs, we found out that your white count and C P K was high, and those abnormalities lined up with serotonin syndrome. What are you experiencing right now?\n",
    "Patient: I have been very restless and easily agitated, I have diarrhea. But no fever or shakiness.\n",
    "Doctor: These can match serotonin syndrome as well. You deny any fever, tremor or hypperflexia so we will give you some IV fluids and I will check on you in an hour or so.\n",
    "Patient: Okay. \n",
    "Doctor: Looks like your C P K counts improved with I V fluids and after discontinuing Prozac.\n",
    "Patient: How are the counts now? Are they normal? Because I feel normal.\n",
    "Doctor: Yes, your C P K and white blood cell counts have come back down. Almost normal now.\n",
    "Patient: My husband left me two weeks ago. My panic attacks are increasing day by day.\n",
    "Doctor: Okay, I see that you have a history of panic attacks and you do have depression and anxiety, is that correct? Last Friday, I talked to psychiatrist about your issues, and he recommended Cymbalta as an alternative to Prozac. \n",
    "Patient: Yes, I stopped taking Prozac, and I am going to see him on Monday or Tuesday. I have a counselor too.\n",
    "Patient: I do think it will be difficult to go home alone but my daughter is coming to visit me in two weeks.\n",
    "Doctor: Oh wow.\n",
    "Patient: Yeah.\n",
    "Doctor: That's nice. Do you have someone who can drop you home and help you?\n",
    "Patient: Yes, I have a friend who does that, I am staying with her for next three days.\n",
    "Doctor: Okay that sounds good. Just continue with your medications for high blood pressure and diabetes as well. So, we treated your imbalance issues and gave you IV fluids, you do not have any more diarrhea, right?\n",
    "Patient: Yes, that's right.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## default model identified 0 entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medspacy\n",
    "nlp = medspacy.load( )\n",
    "doc2 = nlp(text2)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## medical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_sci_scibert\n",
    "import en_core_med7_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IV ROUTE\n",
      "fluids DRUG\n",
      "fluids DRUG\n",
      "Prozac DRUG\n",
      "Cymbalta DRUG\n",
      "Prozac DRUG\n",
      "Prozac DRUG\n",
      "IV ROUTE\n",
      "fluids DRUG\n"
     ]
    }
   ],
   "source": [
    "# import scispacy\n",
    "import spacy\n",
    "\n",
    "nlp2 = spacy.load(\"en_core_med7_trf\")\n",
    "doc2 = nlp2(text2)\n",
    "for ent in doc2.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(IV, fluids, fluids, Prozac, Cymbalta, Prozac, Prozac, IV, fluids)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent recognitioin / classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent recognized: greeting\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to recognize intent\n",
    "def recognize_intent(text):\n",
    "    doc = nlp(text)\n",
    "    # Here you can define your intents based on the entities or patterns\n",
    "    intents = {'greeting': ['hello', 'hi', 'hey'], 'goodbye': ['bye', 'goodbye']}\n",
    "    for token in doc:\n",
    "        for intent, keywords in intents.items():\n",
    "            if token.text.lower() in keywords:\n",
    "                return intent\n",
    "    return 'unknown'\n",
    "\n",
    "# Example usage\n",
    "user_input = 'Hello, how are you?'\n",
    "intent = recognize_intent(user_input)\n",
    "print(f'Intent recognized: {intent}')  # Output: Intent recognized: greeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = [\n",
    "#     ('Hello, I need help', {'entities': [(0, 5, 'greeting')]}),\n",
    "#     ('Goodbye, see you later', {'entities': [(0, 7, 'goodbye')]}),\n",
    "# ]\n",
    "training_data = [\n",
    "    ('Hello, I need help', {'cats': {'greeting': 1.0, 'goodbye': 0.0}}),\n",
    "    ('How is it going', {'cats': {'greeting': 1.0, 'goodbye': 0.0}}),\n",
    "    ('Goodbye, see you later', {'cats': {'greeting': 0.0, 'goodbye': 1.0}}),\n",
    "    ('Byebye, see you', {'cats': {'greeting': 0.0, 'goodbye': 1.0}}),\n",
    "    ('see you', {'cats': {'greeting': 0.0, 'goodbye': 1.0}}),\n",
    "    ('Hi there', {'cats': {'greeting': 1.0, 'goodbye': 0.0}}),\n",
    "    ('See you soon', {'cats': {'greeting': 0.0, 'goodbye': 1.0}}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "# text_cat = nlp.create_pipe('textcat')\n",
    "text_cat = nlp.add_pipe('textcat', last=True)\n",
    "text_cat.add_label('greeting')\n",
    "text_cat.add_label('goodbye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'textcat': 0.25}\n",
      "{'textcat': 0.2427244335412979}\n",
      "{'textcat': 0.23560336232185364}\n",
      "{'textcat': 0.2227870374917984}\n",
      "{'textcat': 0.21068832278251648}\n",
      "{'textcat': 0.18657274544239044}\n",
      "{'textcat': 0.18335475027561188}\n",
      "{'textcat': 0.18716123700141907}\n",
      "{'textcat': 0.1483396738767624}\n",
      "{'textcat': 0.11363311111927032}\n",
      "{'textcat': 0.11102795600891113}\n",
      "{'textcat': 0.09083671122789383}\n",
      "{'textcat': 0.08976726979017258}\n",
      "{'textcat': 0.04922223836183548}\n",
      "{'textcat': 0.062485165894031525}\n",
      "{'textcat': 0.030504679307341576}\n",
      "{'textcat': 0.024628501385450363}\n",
      "{'textcat': 0.01169653795659542}\n",
      "{'textcat': 0.01180785708129406}\n",
      "{'textcat': 0.008775681257247925}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "examples = []\n",
    "for text, annots in training_data:\n",
    "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "nlp.initialize(lambda: examples)\n",
    "\n",
    "n_iter = 20\n",
    "for epoch in range(n_iter):\n",
    "    random.shuffle(examples)\n",
    "    losses = {}\n",
    "    # Create the minibatch generator\n",
    "    for batch in minibatch(examples, size=8):\n",
    "        nlp.update(batch, drop=0.3, losses=losses)\n",
    "    print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: 'Hello there'\n",
      "Prediction scores:\n",
      "greeting: 0.9404430389404297\n",
      "goodbye: 0.059556975960731506\n",
      "Predicted category: greeting\n",
      "\n",
      "Analyzing: 'Goodbye'\n",
      "Prediction scores:\n",
      "greeting: 0.07562782615423203\n",
      "goodbye: 0.924372136592865\n",
      "Predicted category: goodbye\n",
      "\n",
      "Analyzing: 'See you later'\n",
      "Prediction scores:\n",
      "greeting: 0.013240814208984375\n",
      "goodbye: 0.9867592453956604\n",
      "Predicted category: goodbye\n"
     ]
    }
   ],
   "source": [
    "# Assuming trained_nlp is your trained model\n",
    "def predict_text_category(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    print(\"Prediction scores:\")\n",
    "    for label, score in doc.cats.items():\n",
    "        print(f\"{label}: {score}\")\n",
    "    \n",
    "    # Get the category with the highest score\n",
    "    predicted_category = max(doc.cats, key=doc.cats.get)\n",
    "    print(f\"Predicted category: {predicted_category}\")\n",
    "    return predicted_category\n",
    "\n",
    "# Example usage\n",
    "test_texts = [\n",
    "    \"Hello there\",\n",
    "    \"Goodbye\",\n",
    "    \"See you later\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    print(f\"\\nAnalyzing: '{text}'\")\n",
    "    predict_text_category(nlp, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule based solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invain/.virtualenvs/spacy/lib/python3.12/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/invain/.virtualenvs/spacy/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/invain/.virtualenvs/spacy/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/invain/.virtualenvs/spacy/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LOWER\": {\"IN\": [\"print\", \"generate\", \"create\"]}},  # Action keywords\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"*\"},  # Allow intermediate words\n",
    "    {\"LOWER\": \"map\"},\n",
    "    {\"IS_ALPHA\": True, \"OP\": \"*\"},  # Allow intermediate words\n",
    "    {\"LOWER\": {\"IN\": [\"hospital\", \"clinic\", \"station\"]}}  # Target keywords\n",
    "]\n",
    "matcher.add(\"PRINT_MAP\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "can you print a map for the Toronto hospital I was wondering if we could generate one for the Guelph clinic what about printing directions to the nearest gas station\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9095104806068616893 3 10\n",
      "Matched Intent: print a map for the Toronto hospital\n",
      "Descriptor:  hospital\n",
      "9095104806068616893 3 22\n",
      "Matched Intent: print a map for the Toronto hospital I was wondering if we could generate one for the Guelph clinic\n",
      "Descriptor:  hospital\n",
      "Descriptor:  clinic\n",
      "9095104806068616893 3 31\n",
      "Matched Intent: print a map for the Toronto hospital I was wondering if we could generate one for the Guelph clinic what about printing directions to the nearest gas station\n",
      "Descriptor:  hospital\n",
      "Descriptor:  clinic\n",
      "Descriptor:  station\n"
     ]
    }
   ],
   "source": [
    "for matchid, start, end in matches:\n",
    "    print(matchid, start, end)\n",
    "    span = doc[start:end]\n",
    "    print(f\"Matched Intent: {span.text}\")\n",
    "    for token in span:\n",
    "        if token.text.lower() in [\"hospital\", \"clinic\", \"station\"]:\n",
    "            # Check for location descriptors\n",
    "            descriptor = \" \".join(child.text for child in token.lefts if child.dep in [\"compound\", \"amod\"])\n",
    "            print(f\"Descriptor: {descriptor} {token.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings.add(\"coffee\")\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3197928453018144401"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "nlp.vocab.strings[\"coffee\"]\n",
    "nlp.vocab.strings[3197928453018144401]\n",
    "\n",
    "doc = nlp(\"I love coffee\")\n",
    "doc.vocab.strings[\"coffee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('coffee', 3197928453018144401, True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "lexeme.text, lexeme.orth, lexeme.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# Look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# Look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an nlp object\n",
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      "en\n",
      "\n",
      "\n",
      " \n",
      "'\n",
      "''\n",
      "\"\n",
      "'Cause\n",
      "because\n",
      "'cause\n",
      "use\n",
      "'Xxxxx\n",
      "Cause\n",
      "cause\n",
      "C\n",
      "Xxxxx\n",
      "'Cos\n",
      "'cos\n",
      "Cos\n",
      "'Xxx\n",
      "cos\n",
      "Xxx\n",
      "'Coz\n",
      "'coz\n",
      "Coz\n",
      "coz\n",
      "'Cuz\n",
      "'cuz\n",
      "Cuz\n",
      "cuz\n",
      "'S\n",
      "'s\n",
      "'X\n",
      "S\n",
      "s\n",
      "'bout\n",
      "about\n",
      "out\n",
      "'xxxx\n",
      "bout\n",
      "b\n",
      "xxxx\n",
      "c\n",
      "'xxx\n",
      "xxx\n",
      "'d\n",
      "'x\n",
      "d\n",
      "x\n",
      "'em\n",
      "them\n",
      "'xx\n",
      "em\n",
      "e\n",
      "xx\n",
      "'ll\n",
      "will\n",
      "ll\n",
      "l\n",
      "'nuff\n",
      "enough\n",
      "uff\n",
      "nuff\n",
      "n\n",
      "'re\n",
      "are\n",
      "re\n",
      "r\n",
      "(*_*)\n",
      "(\n",
      "_*)\n",
      ")\n",
      "*\n",
      "(-8\n",
      "(-d\n",
      "-8\n",
      "-\n",
      "-d\n",
      "(-:\n",
      ":\n",
      "(-;\n",
      ";\n",
      "(-_-)\n",
      "_-)\n",
      "-_-\n",
      "(._.)\n",
      "_.)\n",
      ".\n",
      "(:\n",
      "(;\n",
      "(=\n",
      "=\n",
      "(>_<)\n",
      "_<)\n",
      ">\n",
      "<\n",
      "(^_^)\n",
      "_^)\n",
      "^_^\n",
      "^\n",
      "(o:\n",
      "(x:\n",
      "o\n",
      "(¬_¬)\n",
      "_¬)\n",
      "¬_¬\n",
      "¬\n",
      "(ಠ_ಠ)\n",
      "_ಠ)\n",
      "(x_x)\n",
      "ಠ_ಠ\n",
      "ಠ\n",
      "x_x\n",
      "(╯°□°）╯︵┻━┻\n",
      "┻━┻\n",
      "┻\n",
      "╯\n",
      "━\n",
      "°\n",
      "□\n",
      "）\n",
      "︵\n",
      ")-:\n",
      "):\n",
      "-__-\n",
      "__-\n",
      "._.\n",
      "0.0\n",
      "0\n",
      "d.d\n",
      "0.o\n",
      "d.x\n",
      "0_0\n",
      "d_d\n",
      "0_o\n",
      "d_x\n",
      "10\n",
      "1\n",
      "dd\n",
      "a.m.\n",
      "a\n",
      ".m.\n",
      "x.x.\n",
      "10a.m\n",
      "a.m\n",
      "ddx.x\n",
      "10a.m.\n",
      "ddx.x.\n",
      "am\n",
      "p.m.\n",
      "p\n",
      "10p.m\n",
      "p.m\n",
      "10p.m.\n",
      "pm\n",
      "11\n",
      "11a.m\n",
      "11a.m.\n",
      "11p.m\n",
      "11p.m.\n",
      "12\n",
      "12a.m\n",
      "12a.m.\n",
      "12p.m\n",
      "12p.m.\n",
      "1a.m\n",
      "dx.x\n",
      "1a.m.\n",
      "dx.x.\n",
      "1p.m\n",
      "1p.m.\n",
      "2\n",
      "2a.m\n",
      "2a.m.\n",
      "2p.m\n",
      "2p.m.\n",
      "3\n",
      "3a.m\n",
      "3a.m.\n",
      "3p.m\n",
      "3p.m.\n",
      "4\n",
      "4a.m\n",
      "4a.m.\n",
      "4p.m\n",
      "4p.m.\n",
      "5\n",
      "5a.m\n",
      "5a.m.\n",
      "5p.m\n",
      "5p.m.\n",
      "6\n",
      "6a.m\n",
      "6a.m.\n",
      "6p.m\n",
      "6p.m.\n",
      "7\n",
      "7a.m\n",
      "7a.m.\n",
      "7p.m\n",
      "7p.m.\n",
      "8)\n",
      "8\n",
      "d)\n",
      "8-)\n",
      "d-)\n",
      "8-\n",
      "d-\n",
      "8-D\n",
      "8-d\n",
      "d-X\n",
      "D\n",
      "8D\n",
      "8d\n",
      "dX\n",
      "8a.m\n",
      "8a.m.\n",
      "8p.m\n",
      "8p.m.\n",
      "9\n",
      "9a.m\n",
      "9a.m.\n",
      "9p.m\n",
      "9p.m.\n",
      ":'(\n",
      ":')\n",
      ":'-(\n",
      "'-(\n",
      ":'-)\n",
      "'-)\n",
      ":(\n",
      ":((\n",
      ":(((\n",
      "(((\n",
      ":()\n",
      ":)\n",
      ":))\n",
      ":)))\n",
      ")))\n",
      ":*\n",
      ":-(\n",
      ":-((\n",
      "-((\n",
      ":-(((\n",
      ":-)\n",
      ":-))\n",
      "-))\n",
      ":-)))\n",
      ":-*\n",
      ":-/\n",
      "-/\n",
      ":-0\n",
      ":-d\n",
      "-0\n",
      ":-3\n",
      "-3\n",
      ":->\n",
      ":-D\n",
      ":-X\n",
      "-D\n",
      "-X\n",
      ":-O\n",
      ":-o\n",
      "-O\n",
      "-o\n",
      ":-P\n",
      ":-p\n",
      "-P\n",
      "-p\n",
      ":-x\n",
      "-x\n",
      ":-]\n",
      "]\n",
      ":-|\n",
      "-|\n",
      ":-}\n",
      "}\n",
      ":/\n",
      "/\n",
      ":0\n",
      ":d\n",
      ":1\n",
      ":3\n",
      ":>\n",
      ":D\n",
      ":X\n",
      ":O\n",
      ":o\n",
      "O\n",
      ":P\n",
      ":p\n",
      "P\n",
      ":x\n",
      ":]\n",
      ":o)\n",
      ":x)\n",
      ":|\n",
      "|\n",
      ":}\n",
      ":’(\n",
      "’\n",
      ":’)\n",
      ":’-(\n",
      "’-(\n",
      ":’-)\n",
      "’-)\n",
      ";)\n",
      ";-)\n",
      ";-D\n",
      ";-d\n",
      ";-X\n",
      ";D\n",
      ";d\n",
      ";X\n",
      ";_;\n",
      "<.<\n",
      "</3\n",
      "</d\n",
      "/3\n",
      "/d\n",
      "<3\n",
      "<d\n",
      "<33\n",
      "<dd\n",
      "33\n",
      "<333\n",
      "333\n",
      "<ddd\n",
      "ddd\n",
      "<space>\n",
      "ce>\n",
      "<xxxx>\n",
      "space\n",
      "ace\n",
      "=(\n",
      "=)\n",
      "=/\n",
      "=3\n",
      "=d\n",
      "=D\n",
      "=X\n",
      "=[\n",
      "[\n",
      "=]\n",
      "=|\n",
      ">.<\n",
      ">.>\n",
      ">:(\n",
      ">:o\n",
      ">:x\n",
      "><(((*>\n",
      "(*>\n",
      "@_@\n",
      "@\n",
      "Adm.\n",
      "adm.\n",
      "A\n",
      "dm.\n",
      "Xxx.\n",
      "Adm\n",
      "adm\n",
      "Ai\n",
      "ai\n",
      "Xx\n",
      "n't\n",
      "not\n",
      "x'x\n",
      "nt\n",
      "n’t\n",
      "x’x\n",
      "Ak.\n",
      "Alaska\n",
      "ak.\n",
      "Xx.\n",
      "Ak\n",
      "ak\n",
      "Ala.\n",
      "Alabama\n",
      "ala.\n",
      "la.\n",
      "Ala\n",
      "ala\n",
      "Apr.\n",
      "April\n",
      "apr.\n",
      "pr.\n",
      "Apr\n",
      "apr\n",
      "Are\n",
      "Ariz.\n",
      "Arizona\n",
      "ariz.\n",
      "iz.\n",
      "Xxxx.\n",
      "Ariz\n",
      "ariz\n",
      "riz\n",
      "Xxxx\n",
      "Ark.\n",
      "Arkansas\n",
      "ark.\n",
      "rk.\n",
      "Ark\n",
      "ark\n",
      "Aug.\n",
      "August\n",
      "aug.\n",
      "ug.\n",
      "Aug\n",
      "aug\n",
      "Bros.\n",
      "bros.\n",
      "B\n",
      "os.\n",
      "Bros\n",
      "bros\n",
      "ros\n",
      "C'm\n",
      "come\n",
      "c'm\n",
      "X'x\n",
      "on\n",
      "C++\n",
      "c++\n",
      "X++\n",
      "Calif.\n",
      "California\n",
      "calif.\n",
      "if.\n",
      "Xxxxx.\n",
      "Calif\n",
      "calif\n",
      "lif\n",
      "Ca\n",
      "can\n",
      "ca\n",
      "'ve\n",
      "have\n",
      "Can\n",
      "ve\n",
      "v\n",
      "’ve\n",
      "’xx\n",
      "Co.\n",
      "co.\n",
      "Co\n",
      "co\n",
      "Colo.\n",
      "Colorado\n",
      "colo.\n",
      "lo.\n",
      "Colo\n",
      "colo\n",
      "olo\n",
      "Conn.\n",
      "Connecticut\n",
      "conn.\n",
      "nn.\n",
      "Conn\n",
      "conn\n",
      "onn\n",
      "Corp.\n",
      "corp.\n",
      "rp.\n",
      "Corp\n",
      "corp\n",
      "orp\n",
      "Could\n",
      "could\n",
      "uld\n",
      "C’m\n",
      "c’m\n",
      "X’x\n",
      "D.C.\n",
      "d.c.\n",
      ".C.\n",
      "X.X.\n",
      "Dare\n",
      "dare\n",
      "Dec.\n",
      "December\n",
      "dec.\n",
      "ec.\n",
      "Dec\n",
      "dec\n",
      "Del.\n",
      "Delaware\n",
      "del.\n",
      "el.\n",
      "Del\n",
      "del\n",
      "Did\n",
      "do\n",
      "did\n",
      "Does\n",
      "does\n",
      "oes\n",
      "Doin\n",
      "doing\n",
      "doin\n",
      "oin\n",
      "Doin'\n",
      "doin'\n",
      "in'\n",
      "Xxxx'\n",
      "Doin’\n",
      "doin’\n",
      "in’\n",
      "Xxxx’\n",
      "Do\n",
      "Dr.\n",
      "dr.\n",
      "Dr\n",
      "dr\n",
      "E.G.\n",
      "e.g.\n",
      "E\n",
      ".G.\n",
      "E.g.\n",
      ".g.\n",
      "X.x.\n",
      "E.g\n",
      "e.g\n",
      "X.x\n",
      "Feb.\n",
      "February\n",
      "feb.\n",
      "F\n",
      "eb.\n",
      "Feb\n",
      "feb\n",
      "Fla.\n",
      "Florida\n",
      "fla.\n",
      "Fla\n",
      "fla\n",
      "Ga.\n",
      "Georgia\n",
      "ga.\n",
      "G\n",
      "Ga\n",
      "ga\n",
      "Gen.\n",
      "gen.\n",
      "en.\n",
      "Gen\n",
      "gen\n",
      "Goin\n",
      "going\n",
      "goin\n",
      "Goin'\n",
      "goin'\n",
      "Goin’\n",
      "goin’\n",
      "Gon\n",
      "gon\n",
      "na\n",
      "to\n",
      "Got\n",
      "got\n",
      "ta\n",
      "t\n",
      "Gov.\n",
      "gov.\n",
      "ov.\n",
      "Gov\n",
      "gov\n",
      "Had\n",
      "had\n",
      "H\n",
      "Has\n",
      "has\n",
      "Have\n",
      "ave\n",
      "Havin\n",
      "having\n",
      "havin\n",
      "vin\n",
      "Havin'\n",
      "havin'\n",
      "Xxxxx'\n",
      "Havin’\n",
      "havin’\n",
      "Xxxxx’\n",
      "He\n",
      "he\n",
      "would\n",
      "He's\n",
      "he's\n",
      "e's\n",
      "Xx'x\n",
      "’d\n",
      "’x\n",
      "’ll\n",
      "’s\n",
      "He’s\n",
      "he’s\n",
      "e’s\n",
      "Xx’x\n",
      "How\n",
      "how\n",
      "'y\n",
      "you\n",
      "How's\n",
      "how's\n",
      "w's\n",
      "Xxx'x\n",
      "’y\n",
      "’re\n",
      "How’s\n",
      "how’s\n",
      "w’s\n",
      "Xxx’x\n",
      "I\n",
      "i\n",
      "'m\n",
      "gonna\n",
      "I.E.\n",
      "i.e.\n",
      ".E.\n",
      "I.e.\n",
      ".e.\n",
      "I.e\n",
      "i.e\n",
      "Ia.\n",
      "Iowa\n",
      "ia.\n",
      "Ia\n",
      "ia\n",
      "Id.\n",
      "Idaho\n",
      "id.\n",
      "Id\n",
      "id\n",
      "Ill.\n",
      "Illinois\n",
      "ill.\n",
      "ll.\n",
      "Ill\n",
      "ill\n",
      "m\n",
      "Inc.\n",
      "inc.\n",
      "nc.\n",
      "Inc\n",
      "inc\n",
      "Ind.\n",
      "Indiana\n",
      "ind.\n",
      "nd.\n",
      "Ind\n",
      "ind\n",
      "Is\n",
      "is\n",
      "It\n",
      "it\n",
      "It's\n",
      "it's\n",
      "t's\n",
      "It’s\n",
      "it’s\n",
      "t’s\n",
      "’m\n",
      "Jan.\n",
      "January\n",
      "jan.\n",
      "J\n",
      "an.\n",
      "Jan\n",
      "jan\n",
      "Jr.\n",
      "jr.\n",
      "Jr\n",
      "jr\n",
      "Jul.\n",
      "July\n",
      "jul.\n",
      "ul.\n",
      "Jul\n",
      "jul\n",
      "Jun.\n",
      "June\n",
      "jun.\n",
      "un.\n",
      "Jun\n",
      "jun\n",
      "Kan.\n",
      "Kansas\n",
      "kan.\n",
      "K\n",
      "Kan\n",
      "kan\n",
      "Kans.\n",
      "kans.\n",
      "ns.\n",
      "Kans\n",
      "kans\n",
      "ans\n",
      "Ky.\n",
      "Kentucky\n",
      "ky.\n",
      "Ky\n",
      "ky\n",
      "La.\n",
      "Louisiana\n",
      "L\n",
      "La\n",
      "la\n",
      "Let\n",
      "let\n",
      "us\n",
      "Let's\n",
      "let's\n",
      "Let’s\n",
      "let’s\n",
      "Lovin\n",
      "loving\n",
      "lovin\n",
      "Lovin'\n",
      "lovin'\n",
      "Lovin’\n",
      "lovin’\n",
      "Ltd.\n",
      "ltd.\n",
      "td.\n",
      "Ltd\n",
      "ltd\n",
      "Ma'am\n",
      "madam\n",
      "ma'am\n",
      "M\n",
      "'am\n",
      "Xx'xx\n",
      "Mar.\n",
      "March\n",
      "mar.\n",
      "ar.\n",
      "Mar\n",
      "mar\n",
      "Mass.\n",
      "Massachusetts\n",
      "mass.\n",
      "ss.\n",
      "Mass\n",
      "mass\n",
      "ass\n",
      "May\n",
      "may\n",
      "Ma’am\n",
      "ma’am\n",
      "’am\n",
      "Xx’xx\n",
      "Md.\n",
      "md.\n",
      "Md\n",
      "md\n",
      "Messrs.\n",
      "messrs.\n",
      "rs.\n",
      "Messrs\n",
      "messrs\n",
      "srs\n",
      "Mich.\n",
      "Michigan\n",
      "mich.\n",
      "ch.\n",
      "Mich\n",
      "mich\n",
      "ich\n",
      "Might\n",
      "might\n",
      "ght\n",
      "Minn.\n",
      "Minnesota\n",
      "minn.\n",
      "Minn\n",
      "minn\n",
      "inn\n",
      "Miss.\n",
      "Mississippi\n",
      "miss.\n",
      "Miss\n",
      "miss\n",
      "iss\n",
      "Mo.\n",
      "mo.\n",
      "Mo\n",
      "mo\n",
      "Mont.\n",
      "mont.\n",
      "nt.\n",
      "Mont\n",
      "mont\n",
      "ont\n",
      "Mr.\n",
      "mr.\n",
      "Mr\n",
      "mr\n",
      "Mrs.\n",
      "mrs.\n",
      "Mrs\n",
      "mrs\n",
      "Ms.\n",
      "ms.\n",
      "Ms\n",
      "ms\n",
      "Mt.\n",
      "Mount\n",
      "mt.\n",
      "Mt\n",
      "mt\n",
      "Must\n",
      "must\n",
      "ust\n",
      "N.C.\n",
      "North Carolina\n",
      "n.c.\n",
      "N\n",
      "N.D.\n",
      "North Dakota\n",
      "n.d.\n",
      ".D.\n",
      "N.H.\n",
      "New Hampshire\n",
      "n.h.\n",
      ".H.\n",
      "N.J.\n",
      "New Jersey\n",
      "n.j.\n",
      ".J.\n",
      "N.M.\n",
      "New Mexico\n",
      "n.m.\n",
      ".M.\n",
      "N.Y.\n",
      "New York\n",
      "n.y.\n",
      ".Y.\n",
      "Neb.\n",
      "Nebraska\n",
      "neb.\n",
      "Neb\n",
      "neb\n",
      "Nebr.\n",
      "nebr.\n",
      "br.\n",
      "Nebr\n",
      "nebr\n",
      "ebr\n",
      "Need\n",
      "need\n",
      "eed\n",
      "Nev.\n",
      "Nevada\n",
      "nev.\n",
      "ev.\n",
      "Nev\n",
      "nev\n",
      "Not\n",
      "Nothin\n",
      "nothing\n",
      "nothin\n",
      "hin\n",
      "Nothin'\n",
      "nothin'\n",
      "Nothin’\n",
      "nothin’\n",
      "Nov.\n",
      "November\n",
      "nov.\n",
      "Nov\n",
      "nov\n",
      "Nuthin\n",
      "nuthin\n",
      "Nuthin'\n",
      "nuthin'\n",
      "Nuthin’\n",
      "nuthin’\n",
      "O'clock\n",
      "o'clock\n",
      "ock\n",
      "X'xxxx\n",
      "O.O\n",
      "o.o\n",
      "X.X\n",
      "O.o\n",
      "O_O\n",
      "o_o\n",
      "X_X\n",
      "O_o\n",
      "X_x\n",
      "Oct.\n",
      "October\n",
      "oct.\n",
      "ct.\n",
      "Oct\n",
      "oct\n",
      "Okla.\n",
      "Oklahoma\n",
      "okla.\n",
      "Okla\n",
      "okla\n",
      "kla\n",
      "Ol\n",
      "old\n",
      "ol\n",
      "Ol'\n",
      "ol'\n",
      "Xx'\n",
      "Ol’\n",
      "ol’\n",
      "Xx’\n",
      "Ore.\n",
      "Oregon\n",
      "ore.\n",
      "re.\n",
      "Ore\n",
      "ore\n",
      "Ought\n",
      "ought\n",
      "O’clock\n",
      "o’clock\n",
      "X’xxxx\n",
      "Pa.\n",
      "Pennsylvania\n",
      "pa.\n",
      "Pa\n",
      "pa\n",
      "Ph.D.\n",
      "ph.d.\n",
      "Xx.X.\n",
      "Ph\n",
      "ph\n",
      "D.\n",
      "d.\n",
      "X.\n",
      "Prof.\n",
      "prof.\n",
      "of.\n",
      "Prof\n",
      "prof\n",
      "rof\n",
      "Rep.\n",
      "rep.\n",
      "R\n",
      "ep.\n",
      "Rep\n",
      "rep\n",
      "Rev.\n",
      "rev.\n",
      "Rev\n",
      "rev\n",
      "S.C.\n",
      "South Carolina\n",
      "s.c.\n",
      "Sen.\n",
      "sen.\n",
      "Sen\n",
      "sen\n",
      "Sep.\n",
      "September\n",
      "sep.\n",
      "Sep\n",
      "sep\n",
      "Sept.\n",
      "sept.\n",
      "pt.\n",
      "Sept\n",
      "sept\n",
      "ept\n",
      "Sha\n",
      "shall\n",
      "sha\n",
      "She\n",
      "she\n",
      "She's\n",
      "she's\n",
      "She’s\n",
      "she’s\n",
      "Should\n",
      "should\n",
      "Somethin\n",
      "something\n",
      "somethin\n",
      "Somethin'\n",
      "somethin'\n",
      "Somethin’\n",
      "somethin’\n",
      "St.\n",
      "st.\n",
      "St\n",
      "st\n",
      "Tenn.\n",
      "Tennessee\n",
      "tenn.\n",
      "T\n",
      "Tenn\n",
      "tenn\n",
      "enn\n",
      "That\n",
      "that\n",
      "hat\n",
      "That's\n",
      "that's\n",
      "Xxxx'x\n",
      "That’s\n",
      "that’s\n",
      "Xxxx’x\n",
      "There\n",
      "there\n",
      "ere\n",
      "There's\n",
      "there's\n",
      "Xxxxx'x\n",
      "There’s\n",
      "there’s\n",
      "Xxxxx’x\n",
      "These\n",
      "these\n",
      "ese\n",
      "They\n",
      "they\n",
      "hey\n",
      "This\n",
      "this\n",
      "his\n",
      "This's\n",
      "this's\n",
      "s's\n",
      "This’s\n",
      "this’s\n",
      "s’s\n",
      "Those\n",
      "those\n",
      "ose\n",
      "V.V\n",
      "v.v\n",
      "V\n",
      "V_V\n",
      "v_v\n",
      "Va.\n",
      "Virginia\n",
      "va.\n",
      "Va\n",
      "va\n",
      "Wash.\n",
      "Washington\n",
      "wash.\n",
      "W\n",
      "sh.\n",
      "Wash\n",
      "wash\n",
      "ash\n",
      "Was\n",
      "was\n",
      "We\n",
      "we\n",
      "Were\n",
      "were\n",
      "What\n",
      "what\n",
      "What's\n",
      "what's\n",
      "What’s\n",
      "what’s\n",
      "When\n",
      "when\n",
      "hen\n",
      "When's\n",
      "when's\n",
      "n's\n",
      "When’s\n",
      "when’s\n",
      "n’s\n",
      "Where\n",
      "where\n",
      "Where's\n",
      "where's\n",
      "Where’s\n",
      "where’s\n",
      "Who\n",
      "who\n",
      "Who's\n",
      "who's\n",
      "o's\n",
      "Who’s\n",
      "who’s\n",
      "o’s\n",
      "Why\n",
      "why\n",
      "Why's\n",
      "why's\n",
      "y's\n",
      "Why’s\n",
      "why’s\n",
      "y’s\n",
      "Wis.\n",
      "Wisconsin\n",
      "wis.\n",
      "is.\n",
      "Wis\n",
      "wis\n",
      "Wo\n",
      "wo\n",
      "Would\n",
      "XD\n",
      "xd\n",
      "XX\n",
      "XDD\n",
      "xdd\n",
      "XXX\n",
      "You\n",
      "Y\n",
      "[-:\n",
      "[:\n",
      "[=\n",
      "\\\")\n",
      "\\\n",
      "\\n\n",
      "\\x\n",
      "\\t\n",
      "]=\n",
      "^__^\n",
      "__^\n",
      "^___^\n",
      "a.\n",
      "x.\n",
      "x.x\n",
      "and/or\n",
      "/or\n",
      "xxx/xx\n",
      "and\n",
      "or\n",
      "b.\n",
      "c.\n",
      "xx.\n",
      "xxxx'\n",
      "xxxx’\n",
      "e.\n",
      "f.\n",
      "f\n",
      "g.\n",
      "g\n",
      "h.\n",
      "h\n",
      "xx'x\n",
      "xx’x\n",
      "xxx'x\n",
      "xxx’x\n",
      "i.\n",
      "j.\n",
      "j\n",
      "k.\n",
      "k\n",
      "l.\n",
      "m.\n",
      "xx'xx\n",
      "xx’xx\n",
      "n.\n",
      "x'xxxx\n",
      "o.\n",
      "o.0\n",
      "x.d\n",
      "o.O\n",
      "x.X\n",
      "o_0\n",
      "x_d\n",
      "o_O\n",
      "x_X\n",
      "xx'\n",
      "xx’\n",
      "x’xxxx\n",
      "p.\n",
      "q.\n",
      "q\n",
      "r.\n",
      "s.\n",
      "t.\n",
      "xxxx'x\n",
      "xxxx’x\n",
      "u.\n",
      "u\n",
      "v.\n",
      "v.s.\n",
      ".s.\n",
      "v.s\n",
      "vs.\n",
      "vs\n",
      "w.\n",
      "w\n",
      "w/o\n",
      "without\n",
      "x/x\n",
      "xD\n",
      "xX\n",
      "xDD\n",
      "xXX\n",
      "y'\n",
      "y\n",
      "x'\n",
      "all\n",
      "y.\n",
      "y’\n",
      "x’\n",
      "z.\n",
      "z\n",
      " \n",
      "  \n",
      "¯\\(ツ)/¯\n",
      "¯\n",
      ")/¯\n",
      "¯\\(x)/¯\n",
      "C.\n",
      "°C.\n",
      "°c.\n",
      "°X.\n",
      "F.\n",
      "°F.\n",
      "°f.\n",
      "K.\n",
      "°K.\n",
      "°k.\n",
      "°x.\n",
      "ä.\n",
      "ä\n",
      "ö.\n",
      "ö\n",
      "ü.\n",
      "ü\n",
      "ಠ︵ಠ\n",
      "x︵x\n",
      "—\n",
      "‘S\n",
      "‘s\n",
      "‘\n",
      "‘X\n",
      "‘x\n",
      "’Cause\n",
      "’cause\n",
      "’Xxxxx\n",
      "’Cos\n",
      "’cos\n",
      "’Xxx\n",
      "’Coz\n",
      "’coz\n",
      "’Cuz\n",
      "’cuz\n",
      "’S\n",
      "’X\n",
      "’bout\n",
      "’xxxx\n",
      "’xxx\n",
      "’em\n",
      "’nuff\n",
      "’’\n"
     ]
    }
   ],
   "source": [
    "for x in spacy.blank(\"en\").vocab.strings:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hello world', 'GREETING')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent = doc.ents[0]\n",
    "ent.text, ent.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\" and doc[token.i + 1].pos_ == \"VERB\":\n",
    "        print(\"Found proper noun before a verb:\", token.text)\n",
    "\n",
    "# for index, pos in enumerate(pos_tags):\n",
    "#     # Check if the current token is a proper noun\n",
    "#     if pos == \"PROPN\":\n",
    "#         # Check if the next token is a verb\n",
    "#         if pos_tags[index + 1] == \"VERB\":\n",
    "#             result = token_texts[index]\n",
    "#             print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invain/.virtualenvs/spacy/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/invain/.virtualenvs/spacy/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/invain/.virtualenvs/spacy/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing: I'm going to prescribe you 500mg of amoxicillin three times a day.\n",
      "Has prescription intent: True\n",
      "Matches found: ['prescribe', 'prescribe you']\n",
      "Dosage information: [{'amount': '500', 'unit': 'mg'}]\n",
      "\n",
      "Analyzing: I recommend taking two tablets of ibuprofen 200mg for your pain.\n",
      "Has prescription intent: True\n",
      "Matches found: ['taking two tablets']\n",
      "Dosage information: [{'amount': 'two', 'unit': 'tablets'}, {'amount': '200', 'unit': 'mg'}]\n",
      "\n",
      "Analyzing: Let's continue with your current medications.\n",
      "Has prescription intent: False\n",
      "\n",
      "Analyzing: I suggest you start taking vitamin D supplements.\n",
      "Has prescription intent: True\n",
      "Matches found: ['suggest you start']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def create_prescription_matcher(nlp: spacy.language.Language) -> Matcher:\n",
    "    \"\"\"Create a matcher with prescription-related patterns\"\"\"\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    # Prescription intent patterns\n",
    "    prescription_patterns = [\n",
    "        # Direct prescription patterns\n",
    "        [\n",
    "            {\"LOWER\": {\"IN\": [\"prescribe\", \"prescribing\", \"prescribed\"]}},\n",
    "            {\"IS_ALPHA\": True, \"OP\": \"*\"},  # Optional words in between\n",
    "            {\"LOWER\": {\"IN\": [\"mg\", \"milligrams\", \"tablets\", \"pills\", \"capsules\"]}, \"OP\": \"?\"}\n",
    "        ],\n",
    "        # Recommendation patterns\n",
    "        [\n",
    "            {\"LOWER\": {\"IN\": [\"recommend\", \"suggesting\", \"suggest\", \"advise\"]}},\n",
    "            {\"IS_ALPHA\": True, \"OP\": \"*\"},\n",
    "            {\"LOWER\": {\"IN\": [\"take\", \"try\", \"start\"]}},\n",
    "        ],\n",
    "        # Direct medication mentions\n",
    "        [\n",
    "            {\"LOWER\": {\"IN\": [\"take\", \"taking\"]}},\n",
    "            {\"IS_ALPHA\": True, \"OP\": \"*\"},\n",
    "            {\"LOWER\": {\"IN\": [\"mg\", \"milligrams\", \"tablets\", \"pills\", \"capsules\"]}}\n",
    "        ],\n",
    "    ]\n",
    "    \n",
    "    matcher.add(\"PRESCRIPTION_INTENT\", prescription_patterns)\n",
    "    return matcher\n",
    "\n",
    "def extract_dosage(doc: spacy.tokens.Doc) -> List[Dict]:\n",
    "    \"\"\"Extract dosage information from text\"\"\"\n",
    "    dosage_info = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        # Look for quantities and measurements\n",
    "        if ent.label_ in [\"QUANTITY\", \"CARDINAL\"]:\n",
    "            next_token = doc[ent.end].text.lower() if ent.end < len(doc) else \"\"\n",
    "            if next_token in [\"mg\", \"milligrams\", \"tablets\", \"pills\", \"capsules\"]:\n",
    "                dosage_info.append({\n",
    "                    \"amount\": ent.text,\n",
    "                    \"unit\": next_token\n",
    "                })\n",
    "    \n",
    "    return dosage_info\n",
    "\n",
    "def analyze_prescription_intent(text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze text for prescription intents and extract relevant information\n",
    "    \n",
    "    Args:\n",
    "        text: The conversation text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing prescription intent information\n",
    "    \"\"\"\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Process text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Create and use matcher\n",
    "    matcher = create_prescription_matcher(nlp)\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # Extract results\n",
    "    result = {\n",
    "        \"has_prescription_intent\": len(matches) > 0,\n",
    "        \"matches\": [],\n",
    "        \"dosage_info\": extract_dosage(doc)\n",
    "    }\n",
    "    \n",
    "    # Get context for each match\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        result[\"matches\"].append({\n",
    "            \"text\": span.text,\n",
    "            \"sentence\": span.sent.text.strip()\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test conversations\n",
    "    conversations = [\n",
    "        \"I'm going to prescribe you 500mg of amoxicillin three times a day.\",\n",
    "        \"I recommend taking two tablets of ibuprofen 200mg for your pain.\",\n",
    "        \"Let's continue with your current medications.\",\n",
    "        \"I suggest you start taking vitamin D supplements.\",\n",
    "    ]\n",
    "    \n",
    "    for conversation in conversations:\n",
    "        result = analyze_prescription_intent(conversation)\n",
    "        print(\"\\nAnalyzing:\", conversation)\n",
    "        print(\"Has prescription intent:\", result[\"has_prescription_intent\"])\n",
    "        if result[\"has_prescription_intent\"]:\n",
    "            print(\"Matches found:\", [m[\"text\"] for m in result[\"matches\"]])\n",
    "            if result[\"dosage_info\"]:\n",
    "                print(\"Dosage information:\", result[\"dosage_info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Losses: {'textcat': 0.5158059000968933}\n",
      "Epoch 1, Losses: {'textcat': 0.48948390781879425}\n",
      "Epoch 2, Losses: {'textcat': 0.4893237352371216}\n",
      "Epoch 3, Losses: {'textcat': 0.48134346306324005}\n",
      "Epoch 4, Losses: {'textcat': 0.48624445497989655}\n",
      "Epoch 5, Losses: {'textcat': 0.47278471291065216}\n",
      "Epoch 6, Losses: {'textcat': 0.4734465926885605}\n",
      "Epoch 7, Losses: {'textcat': 0.4431143254041672}\n",
      "Epoch 8, Losses: {'textcat': 0.4068952649831772}\n",
      "Epoch 9, Losses: {'textcat': 0.3683052062988281}\n",
      "Epoch 10, Losses: {'textcat': 0.31759636104106903}\n",
      "Epoch 11, Losses: {'textcat': 0.3399316221475601}\n",
      "Epoch 12, Losses: {'textcat': 0.20675483345985413}\n",
      "Epoch 13, Losses: {'textcat': 0.194692712277174}\n",
      "Epoch 14, Losses: {'textcat': 0.16266866028308868}\n",
      "Epoch 15, Losses: {'textcat': 0.10904045030474663}\n",
      "Epoch 16, Losses: {'textcat': 0.05978289432823658}\n",
      "Epoch 17, Losses: {'textcat': 0.04654741659760475}\n",
      "Epoch 18, Losses: {'textcat': 0.028156068176031113}\n",
      "Epoch 19, Losses: {'textcat': 0.02325648069381714}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# Training data for prescription intent\n",
    "training_data = [\n",
    "    (\"I'll prescribe you 500mg of amoxicillin\", {'cats': {'prescription': 1.0, 'non_prescription': 0.0}}),\n",
    "    (\"Take two tablets three times a day\", {'cats': {'prescription': 1.0, 'non_prescription': 0.0}}),\n",
    "    (\"I recommend taking this medication\", {'cats': {'prescription': 1.0, 'non_prescription': 0.0}}),\n",
    "    (\"Let's start you on antibiotics\", {'cats': {'prescription': 1.0, 'non_prescription': 0.0}}),\n",
    "    (\"You should take 200mg ibuprofen\", {'cats': {'prescription': 1.0, 'non_prescription': 0.0}}),\n",
    "    (\"How are you feeling today?\", {'cats': {'prescription': 0.0, 'non_prescription': 1.0}}),\n",
    "    (\"Tell me about your symptoms\", {'cats': {'prescription': 0.0, 'non_prescription': 1.0}}),\n",
    "    (\"Your blood pressure looks normal\", {'cats': {'prescription': 0.0, 'non_prescription': 1.0}}),\n",
    "    (\"Let's schedule a follow-up\", {'cats': {'prescription': 0.0, 'non_prescription': 1.0}}),\n",
    "    (\"I'll refer you to a specialist\", {'cats': {'prescription': 0.0, 'non_prescription': 1.0}}),\n",
    "]\n",
    "\n",
    "# Initialize spaCy\n",
    "nlp = spacy.blank('en')\n",
    "text_cat = nlp.add_pipe('textcat', last=True)\n",
    "text_cat.add_label('prescription')\n",
    "text_cat.add_label('non_prescription')\n",
    "\n",
    "# Prepare training examples\n",
    "examples = []\n",
    "for text, annots in training_data:\n",
    "    examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "nlp.initialize(lambda: examples)\n",
    "\n",
    "# Training loop\n",
    "n_iter = 20\n",
    "for epoch in range(n_iter):\n",
    "    random.shuffle(examples)\n",
    "    losses = {}\n",
    "    for batch in minibatch(examples, size=8):\n",
    "        nlp.update(batch, drop=0.3, losses=losses)\n",
    "    print(f\"Epoch {epoch}, Losses:\", losses)\n",
    "\n",
    "def predict_prescription_intent(nlp, text):\n",
    "    doc = nlp(text)\n",
    "    print(\"\\nAnalyzing:\", text)\n",
    "    print(\"Prediction scores:\")\n",
    "    for label, score in doc.cats.items():\n",
    "        print(f\"{label}: {score:.3f}\")\n",
    "    \n",
    "    predicted_category = max(doc.cats, key=doc.cats.get)\n",
    "    print(f\"Predicted category: {predicted_category}\")\n",
    "    return predicted_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing the model:\n",
      "\n",
      "Analyzing: I'm prescribing 20mg of lisinopril daily\n",
      "Prediction scores:\n",
      "prescription: 0.982\n",
      "non_prescription: 0.018\n",
      "Predicted category: prescription\n",
      "\n",
      "Analyzing: Please describe your pain level\n",
      "Prediction scores:\n",
      "prescription: 0.126\n",
      "non_prescription: 0.874\n",
      "Predicted category: non_prescription\n",
      "\n",
      "Analyzing: Take two tablets before bedtime\n",
      "Prediction scores:\n",
      "prescription: 0.987\n",
      "non_prescription: 0.013\n",
      "Predicted category: prescription\n",
      "\n",
      "Analyzing: We'll need to run some tests first\n",
      "Prediction scores:\n",
      "prescription: 0.220\n",
      "non_prescription: 0.780\n",
      "Predicted category: non_prescription\n",
      "\n",
      "Analyzing: Start with 500mg twice daily\n",
      "Prediction scores:\n",
      "prescription: 0.962\n",
      "non_prescription: 0.038\n",
      "Predicted category: prescription\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model\n",
    "test_texts = [\n",
    "    \"I'm prescribing 20mg of lisinopril daily\",\n",
    "    \"Please describe your pain level\",\n",
    "    \"Take two tablets before bedtime\",\n",
    "    \"We'll need to run some tests first\",\n",
    "    \"Start with 500mg twice daily\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting the model:\")\n",
    "for text in test_texts:\n",
    "    predict_prescription_intent(nlp, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "freescribe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
