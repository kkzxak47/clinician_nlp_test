{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use Embeddings to Check for Semantic Consistency\n",
    "Convert both the original conversation and the summary into embeddings.\n",
    "Calculate the similarity score between the summary and the original conversation.\n",
    "If the similarity score is below a threshold, flag the summary as potentially inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkzxa\\AppData\\Local\\Temp\\ipykernel_18468\\3293084087.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "d:\\projects\\clinician_nlp_test\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score: 0.7987412562837334\n",
      "Warning: Possible hallucination or missing details detected!\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# Use a local embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Original conversation\n",
    "conversation = \"Doctor: Your test results are normal. Patient: Great, so no issues? Doctor: Correct, nothing concerning.\"\n",
    "summary = \"Doctor confirmed that the test results are fine.\"\n",
    "\n",
    "# Compute embeddings\n",
    "conversation_embedding = embeddings.embed_query(conversation)\n",
    "summary_embedding = embeddings.embed_query(summary)\n",
    "\n",
    "# Compute similarity (Cosine similarity)\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "similarity_score = cosine_similarity(conversation_embedding, summary_embedding)\n",
    "print(f\"Similarity Score: {similarity_score}\")\n",
    "\n",
    "# Set a threshold (e.g., 0.85)\n",
    "if similarity_score < 0.85:\n",
    "    print(\"Warning: Possible hallucination or missing details detected!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use Textual Comparison for Missing or Added Content\n",
    "Run Named Entity Recognition (NER) to check if any key entities (e.g., names, dates, medications) are missing.\n",
    "Use n-gram overlap to compare word sequences between the original conversation and the summary.\n",
    "Use BLEU or ROUGE scores to measure how much of the original conversation is retained in the summary.\n",
    "Example using ROUGE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score: 0.3478260869565218\n",
      "Warning: Possible missing or altered information in summary!\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "scores = scorer.score(conversation, summary)\n",
    "print(f\"ROUGE Score: {scores['rougeL'].fmeasure}\")\n",
    "\n",
    "if scores['rougeL'].fmeasure < 0.75:\n",
    "    print(\"Warning: Possible missing or altered information in summary!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Retrieve Relevant Passages for Fact-Checking\n",
    "If you have access to a retrieval-based system (e.g., RAG with LangChain), you can:\n",
    "\n",
    "Retrieve key sections of the conversation.\n",
    "Compare them against the LLM-generated summary.\n",
    "Flag any inconsistencies.\n",
    "Example using LangChain Retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant retrieved text: Doctor: Your test results are normal. Patient: Great, so no issues? Doctor: Correct, nothing concerning.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "\n",
    "# Store conversation in vector database\n",
    "vector_db = FAISS.from_texts([conversation], embeddings)\n",
    "\n",
    "# Retrieve top matches to validate the summary\n",
    "retrieved_docs = vector_db.similarity_search(summary, k=1)\n",
    "print(\"Most relevant retrieved text:\", retrieved_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Use Another LLM to Validate the Summary\n",
    "Ask another LLM to verify whether the summary is faithful to the conversation.\n",
    "Provide both the conversation and the summary and ask if any critical details are missing or altered.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, the summary is not fully accurate. \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Missing Context:**  The summary lacks the crucial detail that the doctor stated the results were \"normal\" and there were \"no issues.\" This simple phrase adds important context to the conversation.\n",
      "* **Incomplete Understanding:** While the summary conveys that the test results are \"fine\", it doesn't capture the full meaning of the doctor's response. The patient's question implies they want confirmation that everything is okay and not just a statement about the test results.\n",
      "\n",
      "**A more accurate summary:**\n",
      "\n",
      "> The doctor confirmed that the patient's test results were normal and there were no issues. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Check if the summary is accurate without adding or omitting details.\"),\n",
    "    HumanMessage(content=f\"Conversation: {conversation}\\n\\nSummary: {summary}\\n\\nIs the summary fully accurate?\")\n",
    "]\n",
    "\n",
    "verification = llm(messages)\n",
    "print(verification.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a Python script that integrates multiple hallucination detection techniques to verify whether an LLM-generated summary accurately represents the original conversation. It includes:\n",
    "\n",
    "Embedding Similarity Check (Using OpenAI embeddings)\n",
    "ROUGE Score for Content Overlap\n",
    "Named Entity Recognition (NER) Check (Using SpaCy)\n",
    "LLM-Based Fact Checking\n",
    "Installation Requirements\n",
    "Before running the script, ensure you have the required libraries installed:\n",
    "\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pip install langchain openai faiss-cpu numpy rouge-score spacy\n",
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Embedding Similarity Score: 0.80\n",
      "‚ö†Ô∏è Warning: Possible hallucination detected (Low Similarity Score)!\n",
      "\n",
      "üìè ROUGE Score: 0.35\n",
      "‚ö†Ô∏è Warning: Possible missing or altered information (Low ROUGE Score)!\n",
      "\n",
      "‚úÖ No missing or added named entities detected.\n",
      "\n",
      "\n",
      "ü§ñ LLM Verification Response:\n",
      "Yes, the summary is fully accurate. It captures the essential information from the conversation:\n",
      "\n",
      "* **The doctor confirmed the test results are fine.** This is directly stated in the conversation.\n",
      "* **It avoids adding, missing, or altering details.**  The summary is a concise and accurate representation of the conversation. \n",
      "\n",
      "\n",
      "Let me know if you'd like me to analyze any other summaries! üòä \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load SpaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Use a local embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Define Original Conversation and LLM Summary\n",
    "conversation = \"\"\"\n",
    "Doctor: Your test results are normal.\n",
    "Patient: Great, so no issues?\n",
    "Doctor: Correct, nothing concerning.\n",
    "\"\"\"\n",
    "summary = \"Doctor confirmed that the test results are fine.\"\n",
    "\n",
    "### **1. Embedding Similarity Check**\n",
    "def compute_embedding_similarity(text1, text2):\n",
    "    vec1 = embeddings.embed_query(text1)\n",
    "    vec2 = embeddings.embed_query(text2)\n",
    "    cosine_sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return cosine_sim\n",
    "\n",
    "similarity_score = compute_embedding_similarity(conversation, summary)\n",
    "print(f\"\\nüîç Embedding Similarity Score: {similarity_score:.2f}\")\n",
    "\n",
    "if similarity_score < 0.85:\n",
    "    print(\"‚ö†Ô∏è Warning: Possible hallucination detected (Low Similarity Score)!\\n\")\n",
    "\n",
    "### **2. ROUGE Score for Content Overlap**\n",
    "scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "rouge_score = scorer.score(conversation, summary)[\"rougeL\"].fmeasure\n",
    "print(f\"üìè ROUGE Score: {rouge_score:.2f}\")\n",
    "\n",
    "if rouge_score < 0.75:\n",
    "    print(\"‚ö†Ô∏è Warning: Possible missing or altered information (Low ROUGE Score)!\\n\")\n",
    "\n",
    "### **3. Named Entity Recognition (NER) Check**\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return {ent.text for ent in doc.ents}\n",
    "\n",
    "original_entities = extract_named_entities(conversation)\n",
    "summary_entities = extract_named_entities(summary)\n",
    "\n",
    "missing_entities = original_entities - summary_entities\n",
    "added_entities = summary_entities - original_entities\n",
    "\n",
    "if missing_entities:\n",
    "    print(f\"‚ö†Ô∏è Missing Entities in Summary: {missing_entities}\")\n",
    "if added_entities:\n",
    "    print(f\"‚ö†Ô∏è Added Entities in Summary: {added_entities}\")\n",
    "\n",
    "if not missing_entities and not added_entities:\n",
    "    print(\"‚úÖ No missing or added named entities detected.\\n\")\n",
    "\n",
    "### **4. LLM-Based Fact Checking**\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Check if the summary is accurate without adding, missing, or altering details.\"),\n",
    "    HumanMessage(content=f\"Conversation:\\n{conversation}\\n\\nSummary:\\n{summary}\\n\\nIs the summary fully accurate?\")\n",
    "]\n",
    "\n",
    "verification = llm(messages)\n",
    "print(f\"\\nü§ñ LLM Verification Response:\\n{verification.content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How This Works\n",
    "Embedding Similarity: Compares the semantic similarity between the conversation and the summary.\n",
    "ROUGE Score: Measures content overlap to detect missing information.\n",
    "Named Entity Check: Identifies whether any critical named entities (e.g., names, dates, medications) were added or removed.\n",
    "LLM Verification: Asks GPT-4 if the summary is fully accurate.\n",
    "Interpreting the Results\n",
    "If embedding similarity < 0.85, the summary is significantly different from the conversation.\n",
    "If ROUGE score < 0.75, key information might be missing or altered.\n",
    "If named entities were added or removed, it suggests potential hallucination or omission.\n",
    "If the LLM detects changes, it provides a natural language explanation of what's wrong.\n",
    "Next Steps\n",
    "Would you like to integrate this into your LangChain pipeline, perhaps using a retrieval-based approach (RAG) to fetch trusted documents for comparison?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
