{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"\"\" Hi, Mr. Jones. How are you?  I'm good, Dr. Smith. Nice to see you.  Nice to see you again. What brings you back?  Well, my back's been hurting again.  Oh, I see. I've seen you a number of times for this, haven't I?  Well, ever since I got hurt on the job three years ago, it's something that just keeps coming back.  It'll be fine for a while and then I'll bend down or I'll move in a weird way and then, boom, it'll just go out again.  Unfortunately, that can happen and I do have quite a few patients who get reoccurring episodes of back pain.  Have you been keeping up with the therapy that we had you on before?  Which, the pills?  Actually, I was talking about the physical therapy that we had you doing.  The pills are only meant for short term because they don't actually prevent the back pain from coming back.  Once my back started feeling better, I was happy not to go to the therapist anymore.  Why was that?  Well, it started to become kind of a hassle with my work schedule and the cost was an issue,  but I was able to get back to work and I could use the money.  Do you think the physical therapy was helping?  Yeah, well, it was slow going at first.  I see. Physical therapy is a bit slower than medications, but the point is to build up the core muscles in your back and your abdomen.  Physical therapy is also less invasive than medications, so that's why we had you doing the therapy.  But you mentioned that cost was getting to be a real issue for you. Can you tell me more about that?  Well, the insurance I had only covered a certain number of sessions  and then they moved my therapy office because they were trying to work out my schedule at work,  but that was really far away and then I had to deal with parking and it just started to get really expensive.  Got it. I understand.  So, for now, I'd like you to try using a heating pad for your back pain, so that should help in the short term.  Our goal is to get your back pain under better control without creating additional problems for you like cost.  Let's talk about some different options and the pros and cons of each.  So, the physical therapy is actually really good for your back pain, but there are other things we can be doing to help.  Yes, I definitely don't need to lose any more time at work and just lie around the house all day.  Well, there are some alternative therapies like yoga or tai chi classes or meditation therapies that might be able to help.  And they might also be closer to you and be less expensive. Would that be something you'd be interested in?  Sure, that'd be great.  Good. Let's talk about some of the other costs of your care.  In the past, we had you on some tramadol because the physical therapy alone wasn't working.  Yeah, that medicine was working really well, but again, the cost of it got really expensive.  Yeah, yeah. So, that is something in the future we could order something like a generic medication.  And then there are also resources for people to look up the cheapest cost of their medications.  But for now, I'd like to stick with the non-prescription medications.  And if we can have you go to yoga or tai chi classes, like I mentioned, that could alleviate the need for ordering prescriptions.  Okay, yeah, that sounds good.  Okay, great, great. Are there any other costs that are a problem for you in your care?  Well, my insurance isn't going down, but that seems to be the case for everybody that I talk to.  But I should be able to make it work.  And fortunately, that is an issue for a lot of people.  But I would encourage you during open season to look at your different insurance options to see which plan is more cost effective for you.  Okay. Yeah, that sounds great.  Great, great.  Well, I appreciate you talking to me today.  Yeah, I'm glad you were able to come in.  What I'll do is I'll have my office team research the different things that you and I talked about today.  And then let's set a time early next week, say Tuesday, where we can talk over the phone about what we were able to come up with for you and see if those would work for you.  Okay, great.  Great. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"## SOAP Note\n",
    "\n",
    "**Subjective:** \n",
    "Mr. Jones reports his back pain has been recurring since an injury at work three years ago. He describes it as intermittent, with episodes triggered by bending or unusual movements.  He previously found relief through physical therapy but stopped due to cost and scheduling issues. He acknowledges the effectiveness of tramadol in managing his pain but notes its high cost. Mr. Jones is seeking alternative therapies like yoga or tai chi classes to alleviate his pain and reduce reliance on medications. \n",
    "\n",
    "**Objective:**\n",
    "None.\n",
    "\n",
    "**Assessment:**  \n",
    "Mr. Jones presents with chronic back pain likely related to a previous work injury. He reports intermittent episodes of pain triggered by movement, suggesting potential muscle strain or nerve irritation. His history of seeking physical therapy for this condition highlights the need for ongoing management strategies. \n",
    "\n",
    "**Plan:**\n",
    "1. **Non-prescription medication:**  Recommend continued use of non-prescription medications as a short-term solution while exploring cost-effective options.\n",
    "2. **Alternative therapies:** Encourage Mr. Jones to explore yoga, tai chi, or meditation classes as potential long-term pain management strategies. \n",
    "3. **Insurance review:** Discuss the importance of reviewing insurance plans during open enrollment for more cost-effective coverage options.\n",
    "4. **Phone consultation:** Schedule a follow-up phone call next week to discuss available resources and treatment options in detail.  \n",
    "\n",
    "\n",
    "**Note:** This SOAP note is based solely on the provided conversation and does not include any medical data, vital signs, or lab values. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "def identify_potential_hallucinations(original_text, summary):\n",
    "    \"\"\"\n",
    "    Identifies words that appear in the summary but not in the original text,\n",
    "    which could potentially indicate hallucinations.\n",
    "    \n",
    "    Args:\n",
    "        original_text (str): The original document text\n",
    "        summary (str): The summary to check for hallucinations\n",
    "        \n",
    "    Returns:\n",
    "        list: Words that appear in the summary but not in the original text\n",
    "    \"\"\"\n",
    "    # Download required NLTK resources (uncomment if not already downloaded)\n",
    "    # nltk.download('punkt')\n",
    "    # nltk.download('stopwords')\n",
    "    # nltk.download('wordnet')\n",
    "    \n",
    "    # Initialize lemmatizer and get English stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Function to preprocess text\n",
    "    def preprocess(text):\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Remove punctuation and stopwords, and lemmatize\n",
    "        processed_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in string.punctuation and token not in stop_words:\n",
    "                # Lemmatize to get base form of words\n",
    "                lemma = lemmatizer.lemmatize(token)\n",
    "                processed_tokens.append(lemma)\n",
    "                \n",
    "        return processed_tokens\n",
    "    \n",
    "    # Process both texts\n",
    "    original_tokens = preprocess(original_text)\n",
    "    summary_tokens = preprocess(summary)\n",
    "    \n",
    "    # Find unique tokens in each\n",
    "    original_set = set(original_tokens)\n",
    "    summary_set = set(summary_tokens)\n",
    "    \n",
    "    # Find tokens in summary that aren't in original\n",
    "    potential_hallucinations = summary_set - original_set\n",
    "    \n",
    "    # Get original frequency to provide context\n",
    "    summary_freq = {}\n",
    "    for token in summary_tokens:\n",
    "        if token in potential_hallucinations:\n",
    "            summary_freq[token] = summary_freq.get(token, 0) + 1\n",
    "    \n",
    "    # Sort by frequency\n",
    "    sorted_hallucinations = sorted(\n",
    "        [(word, freq) for word, freq in summary_freq.items()],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    return sorted_hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential hallucinated terms (with frequency):\n",
      "- note: 4\n",
      "- soap: 2\n",
      "- report: 2\n",
      "- injury: 2\n",
      "- intermittent: 2\n",
      "- triggered: 2\n",
      "- movement: 2\n",
      "- seeking: 2\n",
      "- potential: 2\n",
      "- management: 2\n",
      "- strategy: 2\n",
      "- cost-effective: 2\n",
      "- discus: 2\n",
      "- subjective: 1\n",
      "- recurring: 1\n",
      "- describes: 1\n",
      "- bending: 1\n",
      "- unusual: 1\n",
      "- previously: 1\n",
      "- found: 1\n",
      "- relief: 1\n",
      "- stopped: 1\n",
      "- due: 1\n",
      "- scheduling: 1\n",
      "- acknowledges: 1\n",
      "- effectiveness: 1\n",
      "- managing: 1\n",
      "- high: 1\n",
      "- reduce: 1\n",
      "- reliance: 1\n",
      "- objective: 1\n",
      "- none: 1\n",
      "- assessment: 1\n",
      "- present: 1\n",
      "- chronic: 1\n",
      "- likely: 1\n",
      "- related: 1\n",
      "- previous: 1\n",
      "- suggesting: 1\n",
      "- strain: 1\n",
      "- nerve: 1\n",
      "- irritation: 1\n",
      "- history: 1\n",
      "- condition: 1\n",
      "- highlight: 1\n",
      "- ongoing: 1\n",
      "- 1: 1\n",
      "- recommend: 1\n",
      "- continued: 1\n",
      "- short-term: 1\n",
      "- solution: 1\n",
      "- exploring: 1\n",
      "- 2: 1\n",
      "- explore: 1\n",
      "- long-term: 1\n",
      "- 3: 1\n",
      "- review: 1\n",
      "- importance: 1\n",
      "- reviewing: 1\n",
      "- enrollment: 1\n",
      "- coverage: 1\n",
      "- 4: 1\n",
      "- consultation: 1\n",
      "- follow-up: 1\n",
      "- call: 1\n",
      "- available: 1\n",
      "- treatment: 1\n",
      "- detail: 1\n",
      "- based: 1\n",
      "- solely: 1\n",
      "- provided: 1\n",
      "- conversation: 1\n",
      "- include: 1\n",
      "- medical: 1\n",
      "- data: 1\n",
      "- vital: 1\n",
      "- sign: 1\n",
      "- lab: 1\n",
      "- value: 1\n"
     ]
    }
   ],
   "source": [
    "new_words_in_summary = identify_potential_hallucinations(original_text, summary)\n",
    "print(\"Potential hallucinated terms (with frequency):\")\n",
    "for word, freq in new_words_in_summary:\n",
    "    print(f\"- {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra_keywords={'ongoing management strategies', 'discuss available resources', 'intermittent', 'history', '4', 'open enrollment', 'provided conversation', '** alternative therapies :** encourage mr', 'short', 'potential long', 'work three years ago', 'stopped due', 'explore yoga', 'scheduling issues', 'prescription medication :** recommend continued use', 'reports intermittent episodes', 'detail', '** objective :** none', 'based solely', 'jones reports', 'condition highlights', '3', 'high cost', 'previous work injury', 'meditation classes', 'lab values', 'reduce reliance', 'acknowledges', 'vital signs', 'seeking alternative therapies like yoga', 'previously found relief', 'reviewing insurance plans', 'seeking physical therapy', 'effectiveness', 'effective coverage options', 'treatment options', 'jones presents', 'alleviate', '** plan :** 1', '** assessment :** mr', '## soap note ** subjective :** mr', '2', 'effective options', 'bending', 'unusual movements', 'exploring cost', 'term pain management strategies', 'medical data', 'injury', '** phone consultation :** schedule', 'managing', 'movement', 'recurring since', '** non', 'notes', 'suggesting potential muscle strain', 'chronic back pain likely related', '** insurance review :** discuss', 'episodes triggered', 'describes', 'pain triggered', 'nerve irritation', '** note :**', 'tai chi', 'follow', 'include', 'phone call next week', 'pain', 'importance', 'term solution', 'soap note'}\n",
      "4\n",
      "3\n",
      "2\n",
      "pain\n",
      "short\n",
      "notes\n",
      "detail\n",
      "injury\n",
      "** non\n",
      "follow\n",
      "history\n",
      "bending\n",
      "tai chi\n",
      "include\n",
      "managing\n",
      "movement\n",
      "high cost\n",
      "alleviate\n",
      "describes\n",
      "soap note\n",
      "lab values\n",
      "importance\n",
      "stopped due\n",
      "vital signs\n",
      "** note :**\n",
      "intermittent\n",
      "explore yoga\n",
      "based solely\n",
      "acknowledges\n",
      "medical data\n",
      "jones reports\n",
      "effectiveness\n",
      "** plan :** 1\n",
      "term solution\n",
      "potential long\n",
      "jones presents\n",
      "exploring cost\n",
      "pain triggered\n",
      "open enrollment\n",
      "reduce reliance\n",
      "recurring since\n",
      "nerve irritation\n",
      "scheduling issues\n",
      "treatment options\n",
      "effective options\n",
      "unusual movements\n",
      "meditation classes\n",
      "episodes triggered\n",
      "work three years ago\n",
      "condition highlights\n",
      "previous work injury\n",
      "** assessment :** mr\n",
      "phone call next week\n",
      "provided conversation\n",
      "** objective :** none\n",
      "previously found relief\n",
      "seeking physical therapy\n",
      "reviewing insurance plans\n",
      "effective coverage options\n",
      "discuss available resources\n",
      "ongoing management strategies\n",
      "reports intermittent episodes\n",
      "term pain management strategies\n",
      "** insurance review :** discuss\n",
      "chronic back pain likely related\n",
      "## soap note ** subjective :** mr\n",
      "** phone consultation :** schedule\n",
      "suggesting potential muscle strain\n",
      "seeking alternative therapies like yoga\n",
      "** alternative therapies :** encourage mr\n",
      "prescription medication :** recommend continued use\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "rake = Rake()\n",
    "rake.extract_keywords_from_text(original_text)\n",
    "original_keywords = rake.get_ranked_phrases()\n",
    "\n",
    "rake.extract_keywords_from_text(summary)\n",
    "summary_keywords = rake.get_ranked_phrases()\n",
    "\n",
    "\n",
    "extra_keywords = set(summary_keywords) - set(original_keywords)\n",
    "if extra_keywords:\n",
    "    print(f\"{extra_keywords=}\")\n",
    "extra_keywords = list(extra_keywords)\n",
    "extra_keywords.sort(key=lambda x: len(x))\n",
    "for w in extra_keywords:\n",
    "    print(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER (Named Entity Recognition) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\clinician_nlp_test\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New entities in summary: {('3', 'CARDINAL'), ('next week', 'DATE'), ('4', 'CARDINAL'), ('tai chi', 'PERSON'), ('1', 'CARDINAL'), ('2', 'CARDINAL')}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Process the original text and summary\n",
    "original_doc = nlp(original_text)\n",
    "summary_doc = nlp(summary)\n",
    "\n",
    "# Extract entities\n",
    "original_entities = set((ent.text, ent.label_) for ent in original_doc.ents)\n",
    "summary_entities = set((ent.text, ent.label_) for ent in summary_doc.ents)\n",
    "\n",
    "# Find new entities in the summary\n",
    "new_entities = summary_entities - original_entities\n",
    "if new_entities:\n",
    "    print(\"New entities in summary:\", new_entities)\n",
    "else:\n",
    "    print(\"No new entities detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## NER (Named Entity Recognition) with medical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\clinician_nlp_test\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\projects\\clinician_nlp_test\\venv\\lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tramadol DRUG\n"
     ]
    }
   ],
   "source": [
    "# transformers-4.18.0 space 3.4.4\n",
    "# import medspacy\n",
    "import spacy\n",
    "\n",
    "# Load the clinical model (e.g., en_core_sci_sm)\n",
    "nlp = spacy.load(\"en_core_med7_trf\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(original_text)\n",
    "\n",
    "# Extract entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\clinician_nlp_test\\venv\\lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "# import medspacy\n",
    "import spacy\n",
    "\n",
    "# Load the clinical model (e.g., en_core_sci_sm)\n",
    "nlp = spacy.load(\"en_core_med7_trf\")\n",
    "\n",
    "original_doc = nlp(original_text)\n",
    "summary_doc = nlp(summary)\n",
    "\n",
    "original_entities = set((ent.text, ent.label_) for ent in original_doc.ents)\n",
    "summary_entities = set((ent.text, ent.label_) for ent in summary_doc.ents)\n",
    "\n",
    "new_entities = summary_entities - original_entities\n",
    "if new_entities:\n",
    "    print(\"New entities in summary:\", new_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "new_texts = set(ent.text for ent in summary_doc.ents) - set(ent.text for ent in original_doc.ents)\n",
    "print(new_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52.0/52.0 [00:00<?, ?B/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 792/792 [00:00<?, ?B/s] \n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 878k/878k [00:00<00:00, 2.78MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 446k/446k [00:00<00:00, 3.57MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.83G/2.83G [14:16<00:00, 3.55MB/s]\n",
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5352656841278076\n",
      "Recall: 0.5219074487686157\n",
      "F1-score: 0.5285022258758545\n"
     ]
    }
   ],
   "source": [
    "import bert_score\n",
    "\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = bert_score.score([summary], [original_text], lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "\n",
    "print(\"Precision:\", P.item())\n",
    "print(\"Recall:\", R.item())\n",
    "print(\"F1-score:\", F1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîπ How to Interpret BERTScore:\n",
    "\n",
    "F1-score close to 1.0 ‚Üí Very faithful summary (high semantic similarity). ‚úÖ\n",
    "F1-score < 0.7 ‚Üí Possible hallucination (low meaning overlap). ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEURT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\projects\\clinician_nlp_test\\venv\\lib\\site-packages\\bleurt\\score.py:160: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Reading checkpoint BLEURT-20.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:BLEURT-20\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:... vocab_file:None\n",
      "INFO:tensorflow:... do_lower_case:None\n",
      "INFO:tensorflow:... sp_model:sent_piece\n",
      "INFO:tensorflow:... dynamic_seq_length:True\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Will load model: BLEURT-20\\sent_piece.model.\n",
      "INFO:tensorflow:SentencePiece tokenizer created.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEURT Score: 0.42667120695114136\n"
     ]
    }
   ],
   "source": [
    "# pip install git+https://github.com/google-research/bleurt.git\n",
    "\n",
    "# # Downloads the BLEURT-base checkpoint.\n",
    "# wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .\n",
    "# unzip BLEURT-20.zip\n",
    "from bleurt import score\n",
    "\n",
    "# Load BLEURT model (requires download)\n",
    "bleurt_scorer = score.BleurtScorer(\"BLEURT-20\")  # Use a pre-trained checkpoint\n",
    "\n",
    "# Compute BLEURT score\n",
    "bleurt_score = bleurt_scorer.score(references=[original_text], candidates=[summary])\n",
    "print(\"BLEURT Score:\", bleurt_score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîπ How to Interpret BLEURT Scores:\n",
    "\n",
    "BLEURT score > 0.8 ‚Üí The summary is semantically accurate ‚úÖ\n",
    "BLEURT score < 0.5 ‚Üí The summary may contain hallucinations ‚ö†Ô∏è\n",
    "BLEURT score < 0.2 ‚Üí The summary likely introduces false information üö®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLI (Natural Language Inference) Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [00:00<00:00, 728kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.51G/1.51G [07:04<00:00, 3.83MB/s] \n",
      "Some weights of the model checkpoint at microsoft/deberta-large-mnli were not used when initializing DebertaForSequenceClassification: ['config']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52.0/52.0 [00:00<00:00, 51.9kB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 878k/878k [00:00<00:00, 5.06MB/s]\n",
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 446k/446k [00:00<00:00, 4.42MB/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1323 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'ENTAILMENT', 'score': 0.6042574048042297}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nli = pipeline(\"text-classification\", model=\"microsoft/deberta-large-mnli\")\n",
    "\n",
    "\n",
    "result = nli(f\"{original_text} [SEP] {summary}\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sentence-Level Similarity: 0.57772017\n",
      "‚ö†Ô∏è Possible hallucination detected!\n"
     ]
    }
   ],
   "source": [
    "# from transformers import is_torch_tpu_available is deprecated and and was taken out in 4.41.0\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "\n",
    "# Load sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def chunk_and_compare(source_text, summary_text):\n",
    "    # Split source into sentences\n",
    "    sentences = nltk.sent_tokenize(source_text)\n",
    "    \n",
    "    # Encode summary\n",
    "    summary_embedding = model.encode([summary_text])\n",
    "    \n",
    "    # Compute similarity with each sentence\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    similarities = cosine_similarity(sentence_embeddings, summary_embedding)\n",
    "\n",
    "    # Take the highest similarity score\n",
    "    max_similarity = max(similarities)[0]\n",
    "    return max_similarity\n",
    "\n",
    "similarity = chunk_and_compare(original_text, summary)\n",
    "print(\"Max Sentence-Level Similarity:\", similarity)\n",
    "\n",
    "if similarity < 0.7:\n",
    "    print(\"‚ö†Ô∏è Possible hallucination detected!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Top-3 Sentence Similarity: 0.54556143\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def avg_top_k_similarity(source_text, summary_text, k=3):\n",
    "    sentences = nltk.sent_tokenize(source_text)\n",
    "    summary_embedding = model.encode([summary_text])\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    similarities = cosine_similarity(sentence_embeddings, summary_embedding).flatten()\n",
    "\n",
    "    # Take average of top-k most similar sentences\n",
    "    top_k_similarities = np.sort(similarities)[-k:]  # Top k values\n",
    "    avg_similarity = np.mean(top_k_similarities)\n",
    "    \n",
    "    return avg_similarity\n",
    "\n",
    "similarity = avg_top_k_similarity(original_text, summary, k=3)\n",
    "print(\"Avg Top-3 Sentence Similarity:\", similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FactCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT\n",
      "CORRECT\n",
      "CORRECT\n",
      "CORRECT\n",
      "CORRECT\n",
      "INCORRECT\n",
      "INCORRECT\n",
      "INCORRECT\n",
      "CORRECT\n"
     ]
    }
   ],
   "source": [
    "# https://arxiv.org/abs/1910.12840\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = 'manueldeprada/FactCC'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "\n",
    "# input_dict = tokenizer(original_text, summary, max_length=512, padding='max_length', truncation='only_first', return_tensors='pt')\n",
    "# logits = model(**input_dict).logits\n",
    "# pred = logits.argmax(dim=1)\n",
    "# model.config.id2label[pred.item()]\n",
    "\n",
    "def process_long_text(original_text, summary, model, tokenizer, chunk_size=512):\n",
    "    # Split original text into chunks\n",
    "    original_chunks = [original_text[i:i+chunk_size] for i in range(0, len(original_text), chunk_size)]\n",
    "    \n",
    "    results = []\n",
    "    for chunk in original_chunks:\n",
    "        inputs = tokenizer(chunk, summary, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        pred = logits.argmax(dim=1)\n",
    "        print(model.config.id2label[pred.item()])\n",
    "\n",
    "\n",
    "process_long_text(original_text, summary, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DnDScore\n",
    "\n",
    "https://arxiv.org/html/2412.13175v1\n",
    "\n",
    "https://www.youtube.com/watch?v=ry3R7k6x1Pg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newer_transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
